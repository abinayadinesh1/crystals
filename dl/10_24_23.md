inspired by fuyu 8b release paper

bert and gpt2 are **pretrained autoencoding models**
	meaning it was given unlabelled data and generated its own labels for it

 autoencoding models -
	 they themselves have an encoder and a decoder
	 the encoder takes the input image, converts it to a low dimensional version, and then passes it to the decoder who tries to reconstruct the original output
	 how good the model is is based on how well the model can learn low dimensional features of an input ! how representative is the thing they learned of the actual input. so getting only the most important things u need to represent the image is what the decoder optimizes for. 
https://towardsdatascience.com/improving-autoencoder-performance-with-pretrained-rbms-e2e13113c782

apparently its really expensive to pretrain an encoder/decoder model
https://huggingface.co/docs/transformers/model_doc/encoder-decoder

instead, you can warm start your encoder decoder model by giving it a checkpoint/pretrained model to start on. 

BERT and GPT were pretrained transformer based models which is what made them so freaking good.

BERT is an ENCODER ONLY MODEL
![[Screenshot 2023-10-24 at 11.45.56 PM.png]]
bert learns from X1:n to X1:n, and then you can classify sequences by adding a pooling layer then feed forward layer

pooling layer - reduce the spatial dimensions while preserving depth
feed forward layer - opposite of recurrent neural network. 
this consists of a few dense layers which are how the encoder will be able to classify itself into groups. so you freeze the weights for the encoder and then train the dense layers on top so the encoder can now interoperate with the classfications. 

the last layer of the whole model will be some sort of activation function. if it was multiclass classification, use softmax, if it is just two classes, use sigmoid. 